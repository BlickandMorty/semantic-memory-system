Safety-Aware Semantic Memory System

This is a long-term memory (LTM) module for AI agents — basically a semantic memory layer built on ChromaDB + Sentence-Transformers.

The main goal: make memory retrieval useful and safe. In particular, it’s designed to resist memory poisoning, where someone tries to inject misleading “high-importance” memories to steer an agent over time.

⸻

What it does
	•	Stores memories as embeddings for fast semantic lookup
	•	Re-ranks results using a composite score (not just cosine similarity)
	•	Prevents context overflow by tracking token usage
	•	Detects suspicious / repetitive / unverified “memory injections” and reduces their impact

⸻

Key features
	•	Vector embeddings
	•	Uses all-MiniLM-L6-v2 (384-dim) for semantic search
	•	Adversarial defense
	•	Flags near-duplicates and caps importance when a memory looks like an injection attempt
	•	Composite scoring
	•	Final ranking = Similarity (70%) + Importance (30%)
	•	This helps avoid cases where something “semantically close” but irrelevant dominates retrieval
	•	Context budgeting
	•	Estimates token usage and automatically keeps retrieved context within budget

# Safety experiment (memory poisoning)

The demo simulates an attacker trying to insert misinformation by assigning it artificially high importance.

What happens:
	•	the system recognizes the memory as unverified / suspicious
	•	importance is capped / penalized
	•	retrieval score drops, so the agent doesn’t get steered by it

  File structure
	•	semantic_memory.py — core memory engine + scoring logic
	•	demo.py — example run + safety testing
	•	safety_report.json — audit log generated by the demo
